{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the folder path (use raw string for Windows paths)\n",
    "folder = Path(r\"C:\\Users\\Admin\\Music\\stock data\")\n",
    "\n",
    "# 2. Recursively list all files in the folder and subfolders\n",
    "all_files = [file for file in folder.glob(\"**/*\") if file.is_file()]\n",
    "\n",
    "\n",
    "header = True  # Write header only once\n",
    "for file in all_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(r\"C:\\Users\\Admin\\Music\\stock data\\stock_analysis.csv\", mode=\"a\", header=header, index=False)\n",
    "        print(f\"{file} is append\")\n",
    "        header = False  # Skip header after first write\n",
    "\n",
    "\n",
    "print(\"Successfully Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Admin\\Music\\stock data\\stock_analysis.csv\")̥\n",
    "# Group by the 'Ticker' column\n",
    "grouped = df.groupby('Ticker')\n",
    "\n",
    "# Loop through each group\n",
    "for ticker, group_data in grouped:\n",
    "    # Create a filename based on the ticker\n",
    "    filename = rf\"C:\\Users\\Admin\\Music\\stock data\\Tickers\\{ticker}.csv\"\n",
    "    # Save the group data to CSV\n",
    "    group_data.to_csv(filename, index=False)\n",
    "    print(f'{ticker} is saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\" Clean the csv file and insert it into sql \"\"\"\n",
    "\"\"\" remove mmonth column , remmove time from the data coloumn \"\"\"\n",
    "\"\"\" add daily return , sector , cumulativereturn \"\"\" \n",
    "import pandas as pd\n",
    "#load the data set\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\Admin\\Music\\stock data\\stock_analysis.csv\")\n",
    "\n",
    "#Remove duplicate value\n",
    "df = df.drop_duplicates(subset=[\"Ticker\", \"date\"])\n",
    "\n",
    "# Convert to datetime and extract date (ignore time)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date  # Result: 2023-10-\n",
    "\n",
    "#Delete the mmont column\n",
    "df.drop(\"month\", axis=1,inplace=True)\n",
    "\n",
    "#Remove the space\n",
    "df['Ticker']=df[\"Ticker\"].str.strip()\n",
    "\n",
    "#confirm the text size\n",
    "df['Ticker']=df[\"Ticker\"].str.upper()\n",
    "\n",
    "#sort the column for using groupby\n",
    "df = df.sort_values(by=[\"Ticker\", \"date\"])\n",
    "\n",
    "# 2. Group by Ticker and calculate daily returns\n",
    "df[\"Daily_Return\"] = (\n",
    "    df.groupby(\"Ticker\")[\"close\"]\n",
    "    .pct_change()  # Calculate percentage change from previous day\n",
    "    .mul(100)       # Convert to percentage\n",
    "    .round(2)       # Round to 2 decimal places\n",
    ")\n",
    "\n",
    "# 3. Handle NaN (first day of data for each Ticker)\n",
    "df[\"Daily_Return\"] = df[\"Daily_Return\"].fillna(0)  # Optional: Replace NaN with 0%\n",
    "\n",
    "\n",
    "# Calculate cumulative returns (compounded daily returns)\n",
    "df[\"Cumulative_Return\"] = (\n",
    "    df.groupby(\"Ticker\", group_keys=False)[\"Daily_Return\"]  # group_keys=False avoids multi-index\n",
    "    .apply(lambda x: (1 + x / 100).cumprod() - 1) * 100  # Convert back to percentage\n",
    "        ) # Convert back to percentage\n",
    "\n",
    "# Round to 2 decimals\n",
    "df[\"Cumulative_Return\"] = df[\"Cumulative_Return\"].round(2)\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\Admin\\Music\\stock data\\stock_cleaned_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "data=pd.read_csv(r\"C:\\Users\\Admin\\Music\\stock data\\Sector_data.csv\")\n",
    "\n",
    "# Remmove the extra words fromm the symbnol\n",
    "data['Symbol'] = data['Symbol'].str.split(':').str[-1].str.strip()\n",
    "# Remove white spaces\n",
    "data['sector']=data['sector'].str.strip()\n",
    "data['COMPANY']= data['COMPANY'].str.strip()\n",
    "#Save the file\n",
    "data.to_csv(r\"C:\\Users\\Admin\\Music\\stock data\\Sector_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mismatched ticker names (old → new)\n",
    "correction_map = {\n",
    "    \"ADANIGREEN\": \"ADANIENT\",\n",
    "    \"AIRTEL\": \"BHARTIARTL\",\n",
    "    \"TATACONSUMER\": \"TATACONSUM\",\n",
    "   \n",
    "}\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Music\\stock data\\Sector_data.csv\")\n",
    "    \n",
    "    Replace incorrect symbols\n",
    "df[\"Symbol\"] = df[\"Symbol\"].replace(correction_map)\n",
    "    \n",
    "    # Save corrected CSV\n",
    "df.to_csv(r\"C:\\Users\\Admin\\Music\\stock data\\Sector_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import csv\n",
    "\n",
    "\n",
    "# TiDB Cloud connection parameters\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\",\n",
    "    \"port\": 4000,\n",
    "    \"user\": \"4V44XYoMA7okY9v.root\",\n",
    "    \"password\": \"LdkgrMO1WG5Tk9Tk\",\n",
    "    \"database\": \"stock_db\",\n",
    "      \"ssl_verify_cert\": True,\n",
    "  \"ssl_verify_identity\" :  True,\n",
    "  \"cursorclass\": pymysql.cursors.DictCursor\n",
    "}\n",
    "\n",
    "def insert_tickers_data(tickers_csv_path):\n",
    "    \"\"\"Insert ticker data with TiDB Cloud connection and SSL\"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = pymysql.connect(**DB_CONFIG)\n",
    "        with connection.cursor() as cursor:\n",
    "            with open(tickers_csv_path, 'r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                \n",
    "                for row in reader:\n",
    "                    symbol = row['Symbol'].strip()\n",
    "                    company = row['COMPANY'].strip()\n",
    "                    sector = row['sector'].strip()\n",
    "\n",
    "                    # Check existing\n",
    "                    cursor.execute(\"SELECT ticker_id FROM tickers WHERE ticker_name = %s\", (symbol,))\n",
    "                    if not cursor.fetchone():\n",
    "                        cursor.execute(\"\"\"\n",
    "                            INSERT INTO tickers (ticker_name, company_name, sector)\n",
    "                            VALUES (%s, %s, %s)\n",
    "                        \"\"\", (symbol, company, sector))\n",
    "                        print(f\"Inserted: {symbol}\")\n",
    "                        \n",
    "            connection.commit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "            print(\"Tickers connection closed\")\n",
    "\n",
    "def insert_stock_data(stock_csv_path):\n",
    "    \"\"\"Insert stock data with ticker_id mapping and all required columns\"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = pymysql.connect(**DB_CONFIG)\n",
    "        with connection.cursor() as cursor:\n",
    "            # Get ticker mapping from database\n",
    "            cursor.execute(\"SELECT ticker_id, ticker_name FROM tickers\")\n",
    "            ticker_map = {row['ticker_name']: row['ticker_id'] for row in cursor.fetchall()}\n",
    "\n",
    "            # Insert stock data\n",
    "            with open(stock_csv_path, 'r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                \n",
    "                for row in reader:\n",
    "                    # Extract and clean values\n",
    "                    ticker_name = row['Ticker'].strip()\n",
    "                    date = row['date']\n",
    "                    \n",
    "                    # Skip if ticker not found\n",
    "                    if ticker_name not in ticker_map:\n",
    "                        print(f\"Skipping: {ticker_name} not found\")\n",
    "                        continue\n",
    "\n",
    "                    # Prepare data for insertion\n",
    "                    stock_data = {\n",
    "                        'ticker_id': ticker_map[ticker_name],\n",
    "                        'date': date,\n",
    "                        'open': row.get('open'),\n",
    "                        'high': row.get('high'),\n",
    "                        'low': row.get('low'),\n",
    "                        'close': row.get('close'),\n",
    "                        'volume': row.get('volume'),\n",
    "                        'daily_return': row.get('Daily_Return'),\n",
    "                        'cumulative_return': row.get('Cumulative_Return')\n",
    "                    }\n",
    "\n",
    "                    # Convert numeric values\n",
    "                    for key in ['open', 'high', 'low', 'close', 'volume', \n",
    "                               'daily_return', 'cumulative_return']:\n",
    "                        if stock_data[key]:\n",
    "                            stock_data[key] = float(stock_data[key])\n",
    "\n",
    "                    # Check for existing entry\n",
    "                    cursor.execute(\"\"\"\n",
    "                        SELECT data_id FROM stock_data \n",
    "                        WHERE ticker_id = %s AND date = %s\n",
    "                    \"\"\", (stock_data['ticker_id'], stock_data['date']))\n",
    "                    \n",
    "                    if not cursor.fetchone():\n",
    "                        # Insert new record\n",
    "                        cursor.execute(\"\"\"\n",
    "                            INSERT INTO stock_data \n",
    "                            (ticker_id, date, open, high, low, close, \n",
    "                             volume, daily_return, cumulative_return)\n",
    "                            VALUES (%(ticker_id)s, %(date)s, %(open)s, %(high)s, \n",
    "                                    %(low)s, %(close)s, %(volume)s, \n",
    "                                    %(daily_return)s, %(cumulative_return)s)\n",
    "                        \"\"\", stock_data)\n",
    "                        \n",
    "                        print(f\"Inserted: {ticker_name} - {date} - {stock_data['close']}\")\n",
    "            \n",
    "            connection.commit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "            print(\"Stock data connection closed\")\n",
    "\n",
    "Execute with your specific file paths\n",
    "tickers_path = r\"C:\\Users\\Admin\\Music\\stock data\\Sector_data.csv\"\n",
    "stock_path = r\"C:\\Users\\Admin\\Music\\stock data\\stock_cleaned_data.csv\"\n",
    "insert_tickers_data(tickers_path)\n",
    "insert_stock_data(stock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TiDB Cloud connection parameters\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\",\n",
    "    \"port\": 4000,\n",
    "    \"user\": \"4V44XYoMA7okY9v.root\",\n",
    "    \"password\": \"LdkgrMO1WG5Tk9Tk\",\n",
    "    \"database\": \"stock_db\",\n",
    "      \"ssl_verify_cert\": True,\n",
    "  \"ssl_verify_identity\" :  True,\n",
    "  \"cursorclass\": pymysql.cursors.DictCursor\n",
    "}\n",
    "\n",
    "\n",
    "def insert_britannia_data(stock_csv_path):\n",
    "    \"\"\"Insert Britannia stock data with fixed ticker_id=26\"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = pymysql.connect(**DB_CONFIG)\n",
    "        with connection.cursor() as cursor:\n",
    "            # Filter and insert Britannia data\n",
    "            with open(stock_csv_path, 'r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                britannia_rows = [row for row in reader if row['Ticker'].strip().upper() == 'BRITANNIA']\n",
    "                \n",
    "                print(f\"Found {len(britannia_rows)} Britannia records to process\")\n",
    "                \n",
    "                for row in britannia_rows:\n",
    "\n",
    "                    display(row.get(\"close\"))\n",
    "                    Prepare data with fixed ticker_id=26\n",
    "                    stock_data = {\n",
    "                        'ticker_id': 26,  # Hardcoded based on your ticker table\n",
    "                        'date': row['date'],\n",
    "                        'open': float(row.get('open', 0)),\n",
    "                        'high': float(row.get('high', 0)),\n",
    "                        'low': float(row.get('low', 0)),\n",
    "                        'close': float(row.get('close', 0)),\n",
    "                        'volume': int(float(row.get('volume', 0))),  # Handle decimal volumes\n",
    "                        'daily_return': float(row.get('Daily_Return', 0)),\n",
    "                        'cumulative_return': float(row.get('Cumulative_Return', 0))\n",
    "                    }\n",
    "\n",
    "                    # Check for existing entry\n",
    "                    cursor.execute(\"\"\"\n",
    "                        SELECT data_id FROM stock_data \n",
    "                        WHERE ticker_id = %s AND date = %s\n",
    "                    \"\"\", (stock_data['ticker_id'], stock_data['date']))\n",
    "                    \n",
    "                    if not cursor.fetchone():\n",
    "                        cursor.execute(\"\"\"\n",
    "                            INSERT INTO stock_data \n",
    "                            (ticker_id, date, open, high, low, close, \n",
    "                             volume, daily_return, cumulative_return)\n",
    "                            VALUES (%(ticker_id)s, %(date)s, %(open)s, %(high)s, \n",
    "                                    %(low)s, %(close)s, %(volume)s, \n",
    "                                    %(daily_return)s, %(cumulative_return)s)\n",
    "                        \"\"\", stock_data)\n",
    "                        \n",
    "                        print(f\"Inserted Britannia - {stock_data['date']} - Close: {stock_data['close']}\")\n",
    "            \n",
    "            connection.commit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "            print(\"Britannia data connection closed\")\n",
    "\n",
    "# Execute with your path\n",
    "stock_path = r\"C:\\Users\\Admin\\Music\\stock data\\stock_cleaned_data.csv\"\n",
    "insert_britannia_data(stock_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
